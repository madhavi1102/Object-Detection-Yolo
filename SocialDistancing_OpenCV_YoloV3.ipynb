{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SocialDistancing_OpenCV_YoloV3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lxC4VSoeR20"
      },
      "source": [
        "#Download the yolov3 weights\n",
        "!wget https://pjreddie.com/media/files/yolov3.weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAfnXK4p7mF8"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26Q6kfCn7_VR"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dVupclS37sm"
      },
      "source": [
        "### Saved sample test vedio file in my google drive. Mount the google drive to access that file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85fP3S-eh-OF",
        "outputId": "36417753-6f5c-4f80-d823-86f2d9ac3afc"
      },
      "source": [
        "# mount my google drive to access mp4 vedio file stored in my google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/mydrive\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/mydrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWT-7xtcirMu"
      },
      "source": [
        "testmp4=\"/content/mydrive/MyDrive/Yolo_ObjectDetection/pedestrians.mp4\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qeq-syAdixQV"
      },
      "source": [
        "# Setting minimum value for the confidence score of boundary box\n",
        "#setting minimum threshold value for non-max suppression to remove the overlapping bounding boxes\n",
        "MIN_CONF = 0.3\n",
        "NMS_THRESH = 0.3\n",
        "\n",
        "# define the minimum safe distance (in pixels) that two people can be\n",
        "# from each other\n",
        "MIN_DISTANCE = 50"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idYBU6ovjdpG"
      },
      "source": [
        "### Defining people detection function using yolov3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4fRN695ixNN"
      },
      "source": [
        "import numpy as np\n",
        "import cv2"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls-tNBRXixHl"
      },
      "source": [
        "\"\"\"People Detection function to get coordinates of bounding boxes resizing  to cover the persons. \n",
        "It returns the list of list of confidence scores for the class,left-top and bottom-right x y coordinates of \n",
        "the bounding box and centeroid points for a bounding box.\"\"\"\n",
        "\n",
        "def detect_people(frame,net,ln,personIdx):\n",
        "  H,W=frame.shape[:2]\n",
        "  results=[]\n",
        "\n",
        "  blob=cv2.dnn.blobFromImage(frame,1/255.0,(416,416),swapRB=True,crop=False)\n",
        "  net.setInput(blob)\n",
        "  layerOutputs = net.forward(ln)\n",
        "\n",
        "  boxes=[]\n",
        "  centroids=[]\n",
        "  confidences=[]\n",
        "\n",
        "  for output in layerOutputs:\n",
        "    for detection in output:\n",
        "      scores=detection[5:]\n",
        "      classId=np.argmax(scores)\n",
        "      confidence=scores[classId]\n",
        "\n",
        "      if classId==personIdx and confidence >MIN_CONF:\n",
        "        # defining the bounding box to fit the image\n",
        "        box=detection[0:4]*np.array([W,H,W,H])\n",
        "        center_x,center_y,width,height=box.astype(\"int\")\n",
        "\n",
        "        leftx=int(center_x-(width/2))\n",
        "        lefty=int(center_y-(height/2))\n",
        "\n",
        "        boxes.append([leftx,lefty,int(width),int(height)])\n",
        "        centroids.append((center_x,center_y))\n",
        "        confidences.append(float(confidence))\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    # apply non-maxima suppression to suppress weak, overlapping\n",
        "\t# bounding boxes\n",
        "  idxs=cv2.dnn.NMSBoxes(boxes,confidences,MIN_CONF,NMS_THRESH)\n",
        "\n",
        "  if len(idxs)>0:\n",
        "    for i in idxs.flatten():\n",
        "      x,y=(boxes[i][0],boxes[i][1])\n",
        "      w,h=(boxes[i][2],boxes[i][3])\n",
        "\n",
        "      r=(confidences[i],(x,y,x+w,y+h), centroids[i])\n",
        "      results.append(r)\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOKNK96kuTzG"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "from scipy.spatial import distance as dist\n",
        "import numpy as np\n",
        "import argparse\n",
        "import imutils\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRSzvawyuTtX",
        "outputId": "44956277-809d-4348-af61-e878e4474617"
      },
      "source": [
        "# construct the argument parse and parse the arguments\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"-i\", \"--input\", type=str, default=\"\",\n",
        "\thelp=\"path to (optional) input video file\")\n",
        "\n",
        "ap.add_argument(\"-o\", \"--output\", type=str, default=\"\",\n",
        "\thelp=\"path to (optional) output video file\")\n",
        "\n",
        "ap.add_argument(\"-d\", \"--display\", type=int, default=1,\n",
        "\thelp=\"whether or not output frame should be displayed\")\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['-d', '--display'], dest='display', nargs=None, const=None, default=1, type=<class 'int'>, choices=None, help='whether or not output frame should be displayed', metavar=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEe2BS0Iv0E3"
      },
      "source": [
        "args = vars(ap.parse_args([\"--input\",\"/content/mydrive/MyDrive/Yolo_ObjectDetection/pedestrians.mp4\",\"--output\",\"my_output.avi\",\"--display\",\"1\"]))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSIiyLtMwFkZ"
      },
      "source": [
        "# load the COCO class labels our YOLO model was trained on\n",
        "labelsPath = \"/content/coco.names\"\n",
        "LABELS = open(labelsPath).read().strip().split(\"\\n\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPmsHRkNwWA4",
        "outputId": "da217f16-3a5f-452a-d1b8-7bc8ce0120d1"
      },
      "source": [
        "print(LABELS)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSzD5q91xPjA"
      },
      "source": [
        "## loading pretrained yolov3 model\n",
        "net=cv2.dnn.readNetFromDarknet(\"/content/yolov3.cfg\",\"/content/yolov3.weights\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNBDhDB4xPa5"
      },
      "source": [
        "# determine only the *output* layer names that we need from YOLO\n",
        "ln = net.getLayerNames()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evSR_bbVzi0y",
        "outputId": "241d038f-67a4-4560-fb82-3c75d962dfa7"
      },
      "source": [
        "print(net.getUnconnectedOutLayers())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[200]\n",
            " [227]\n",
            " [254]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6up7okoyxvex"
      },
      "source": [
        "## detection layers name\n",
        "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQVfmjY4zu0c",
        "outputId": "4a912861-ca83-4dbd-c17e-e288299fb09b"
      },
      "source": [
        "ln"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['yolo_82', 'yolo_94', 'yolo_106']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UxRnCXizxzS",
        "outputId": "1da07e43-3701-4e82-9383-445e2b1b7292"
      },
      "source": [
        "# initialize the video stream and pointer to output video file\n",
        "print(\"[INFO] accessing video stream...\")\n",
        "vs = cv2.VideoCapture(args[\"input\"] if args[\"input\"] else 0)\n",
        "writer = None"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] accessing video stream...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo_mqYsZ0AZD"
      },
      "source": [
        "# loop over the frames from the video stream\n",
        "while True:\n",
        "\t# read the next frame from the file\n",
        "\t(grabbed, frame) = vs.read()\n",
        "\n",
        "\t# if the frame was not grabbed, then we have reached the end\n",
        "\t# of the stream\n",
        "\tif not grabbed:\n",
        "\t\tbreak\n",
        "\n",
        "\t# resize the frame and then detect people (and only people) in it\n",
        "\tframe = imutils.resize(frame, width=700)\n",
        "\tresults = detect_people(frame, net, ln,personIdx=LABELS.index(\"person\"))\n",
        "\n",
        "\t# initialize the set of indexes that violate the minimum social\n",
        "\t# distance\n",
        "\tviolate = set()\n",
        "\n",
        "\t# ensure there are *at least* two people detections (required in\n",
        "\t# order to compute our pairwise distance maps)\n",
        "\tif len(results) >= 2:\n",
        "\t\t# extract all centroids from the results and compute the\n",
        "\t\t# Euclidean distances between all pairs of the centroids\n",
        "\t\tcentroids = np.array([r[2] for r in results])\n",
        "\t\tD = dist.cdist(centroids, centroids, metric=\"euclidean\")\n",
        "\n",
        "\t\t# loop over the upper triangular of the distance matrix\n",
        "\t\tfor i in range(0, D.shape[0]):\n",
        "\t\t\tfor j in range(i + 1, D.shape[1]):\n",
        "\t\t\t\t# check to see if the distance between any two\n",
        "\t\t\t\t# centroid pairs is less than the configured number\n",
        "\t\t\t\t# of pixels\n",
        "\t\t\t\tif D[i, j] < MIN_DISTANCE:\n",
        "\t\t\t\t\t# update our violation set with the indexes of\n",
        "\t\t\t\t\t# the centroid pairs\n",
        "\t\t\t\t\tviolate.add(i)\n",
        "\t\t\t\t\tviolate.add(j)\n",
        "\n",
        "\t# loop over the results\n",
        "\tfor (i, (prob, bbox, centroid)) in enumerate(results):\n",
        "\t\t# extract the bounding box and centroid coordinates, then\n",
        "\t\t# initialize the color of the annotation\n",
        "\t\t(startX, startY, endX, endY) = bbox\n",
        "\t\t(cX, cY) = centroid\n",
        "\t\tcolor = (0, 255, 0)\n",
        "\n",
        "\t\t# if the index pair exists within the violation set, then\n",
        "\t\t# update the color\n",
        "\t\tif i in violate:\n",
        "\t\t\tcolor = (0, 0, 255)\n",
        "\n",
        "\t\t# draw (1) a bounding box around the person and (2) the\n",
        "\t\t# centroid coordinates of the person,\n",
        "\t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
        "\t\tcv2.circle(frame, (cX, cY), 2, color, 1)\n",
        "\n",
        "\t# draw the total number of social distancing violations on the\n",
        "\t# output frame\n",
        "\ttext = \"Social Distancing Violations: {}\".format(len(violate))\n",
        "\tcv2.putText(frame, text, (10, frame.shape[0] - 25),\n",
        "\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.85, (0, 0, 255), 3)\n",
        "\n",
        "\t# check to see if the output frame should be displayed to our\n",
        "\t# screen\n",
        "\tif args[\"display\"] > 0:\n",
        "\t\t# show the output frame\n",
        "\t\tcv2_imshow(frame)\n",
        "\t\tkey = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "\t\t# if the `q` key was pressed, break from the loop\n",
        "\t\tif key == ord(\"q\"):\n",
        "\t\t\tbreak\n",
        "\n",
        "\t# if an output video file path has been supplied and the video\n",
        "\t# writer has not been initialized, do so now\n",
        "\tif args[\"output\"] != \"\" and writer is None:\n",
        "\t\t# initialize our video writer\n",
        "\t\tfourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
        "\t\twriter = cv2.VideoWriter(args[\"output\"], fourcc, 25,\n",
        "\t\t\t(frame.shape[1], frame.shape[0]), True)\n",
        "\n",
        "\t# if the video writer is not None, write the frame to the output\n",
        "\t# video file\n",
        "\tif writer is not None:\n",
        "\t\twriter.write(frame)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiXrFG9m8bFR"
      },
      "source": [
        "Sources:\n",
        "\n",
        "https://blog.paperspace.com/how-to-implement-a-yolo-v3-object-detector-from-scratch-in-pytorch-part-2/\n",
        "\n",
        "https://github.com/abd-shoumik/Social-distance-detection"
      ]
    }
  ]
}